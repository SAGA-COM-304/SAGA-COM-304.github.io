<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>nano4M – Minimalist Multimodal Prototype</title>
  <meta name="description" content="nano4M – COM-304 project page. Dataset pipeline, tokenisers and preliminary multimodal Transformer results." />

  <!-- Open Graph -->
  <meta property="og:title" content="nano4M – Minimalist Multimodal Prototype" />
  <meta property="og:description" content="COM-304 project page: dataset pipeline, tokenisers and early results." />
  <meta property="og:image" content="images/og_cover.jpg" />
  <meta property="og:type" content="website" />

  <!-- Fonts & Tailwind -->
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600;700&display=swap" rel="stylesheet" />
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      darkMode: "class",
      theme: {
        extend: {
          colors: {
            brand: {
              50: "#f2f6ff",
              100: "#e6edff",
              400: "#60a5fa",
              500: "#1e40af",
              600: "#1b37a0",
            },
          },
          fontFamily: { sans: ["Poppins", "sans-serif"] },
        },
      },
    };
  </script>

  <!-- Utility tweaks -->
  <style>
    .nav-link{position:relative;padding-bottom:4px;transition:color .3s;}
    .nav-link::after{content:"";position:absolute;width:0;height:2px;bottom:0;left:0;background:currentColor;transition:width .3s;}
    .nav-link:hover::after,[aria-current="page"].nav-link::after{width:100%;}
    .card-hover{transition:transform .25s,box-shadow .25s;}
    .card-hover:hover{transform:translateY(-6px);box-shadow:0 12px 24px rgba(0,0,0,.15);}
    ::-webkit-scrollbar{width:8px;}::-webkit-scrollbar-thumb{background:#6b7280;border-radius:9999px;}
  </style>
</head>

<body class="bg-white text-gray-800 dark:bg-gray-900 dark:text-gray-200 font-sans relative">
  <!-- Skip link -->
  <a href="#main" class="sr-only focus:not-sr-only absolute top-0 left-0 m-2 px-3 py-2 rounded-lg bg-white text-brand-600 dark:bg-gray-800 dark:text-brand-400 z-50">Skip to content</a>

  <!-- Navbar -->
  <header class="sticky top-0 z-50 w-full backdrop-blur bg-white/90 dark:bg-gray-900/90 shadow-md">
    <div class="max-w-7xl mx-auto flex items-center justify-between px-4 py-3">
      <a href="https://www.epfl.ch" target="_blank" rel="noopener noreferrer" class="flex items-center gap-2" aria-label="EPFL home">
        <img src="https://www.epfl.ch/wp-content/themes/wp-theme-2018/assets/svg/epfl-logo.svg" alt="EPFL logo" class="h-8" loading="lazy" />
      </a>

      <nav class="hidden md:flex items-center space-x-6 lg:space-x-8 text-sm font-medium">
        <a href="#introduction" class="nav-link">Introduction</a>
        <a href="#capabilities" class="nav-link">Capabilities</a>
        <a href="#extensions" class="nav-link">Extensions</a>
        <a href="#results" class="nav-link">Results</a>
        <a href="#conclusion" class="nav-link">Conclusion</a>
        <a href="#references" class="nav-link">References</a>
      </nav>

      <div class="flex items-center gap-3">
        <!-- Dark mode toggle -->
        <button id="dark-toggle" class="p-2 rounded-lg hover:bg-gray-100 dark:hover:bg-gray-800 focus:outline-none" aria-label="Toggle dark mode">
          <svg id="icon-sun" class="h-5 w-5 hidden" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 18.5A6.5 6.5 0 1 0 12 5.5a6.5 6.5 0 0 0 0 13z" />
          </svg>
          <svg id="icon-moon" class="h-5 w-5 hidden" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 1 1 8.646 3.646a7 7 0 1 0 11.708 11.708z" />
          </svg>
        </button>

        <!-- Burger -->
        <button id="menu-button" class="md:hidden p-2 rounded-lg hover:bg-gray-100 dark:hover:bg-gray-800 focus:outline-none" aria-label="Open menu">
          <svg class="h-6 w-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 8h16M4 16h16" />
          </svg>
        </button>
      </div>
    </div>

    <!-- Mobile nav -->
    <nav id="mobile-nav" class="md:hidden max-h-0 overflow-hidden transition-[max-height] duration-300 text-sm font-medium px-4">
      <div class="flex flex-col py-3 gap-3">
        <a href="#introduction" class="nav-link">Introduction</a>
        <a href="#methodology" class="nav-link">Methodology</a>
        <a href="#capabilities" class="nav-link">Capabilities</a>
        <a href="#extensions" class="nav-link">Extensions</a>
        <a href="#results" class="nav-link">Results</a>
        <a href="#conclusion" class="nav-link">Conclusion</a>
        <a href="#references" class="nav-link">References</a>
      </div>
    </nav>
  </header>

  <!-- Hero -->
  <section class="relative overflow-hidden bg-brand-500 text-white">
    <svg class="absolute inset-0 w-full h-full object-cover opacity-20" preserveAspectRatio="xMidYMid slice" viewBox="0 0 1463 360" aria-hidden="true">
      <path fill="currentColor" d="M-82.673 72.249c70.665-4.183 134.833 41.899 200.73 72.997 100.895 47.676 217.861 63.259 316.704 14.985 138.659-65.846 254.091-245.721 396.898-240.006 105.89 4.271 171.479 127.1 254.258 186.087 93.196 66.424 216.399 56.62 326.026 38.115 36.249-5.993 147.36-48.106 160.415-15.18C1603.27 242.96 1256.46 471.05 950.12 488.23 646.025 505.218 458.88 310.348 219.58 287.278 133.329 279.12 32.544 328.913-39.303 278.6c-66.829-47.576-50.224-179.8 56.63-206.35z"/>
    </svg>
    <div class="relative max-w-7xl mx-auto px-4 py-24 text-center">
      <h1 class="text-4xl sm:text-5xl lg:text-6xl font-extrabold mb-4">nano4M Project</h1>
      <p class="max-w-2xl mx-auto text-lg sm:text-xl mb-8">A minimalist multimodal prototype built for EPFL's COM-304 course.</p>
      <div class="flex flex-col sm:flex-row justify-center gap-4">
        <a href="#capabilities" class="inline-block px-6 py-3 text-base font-semibold rounded-lg bg-white/10 backdrop-blur hover:bg-white/20 border border-white">Discover more</a>
        <a href="https://github.com/SAGA-COM-304/SAGA_COM-304" target="_blank" rel="noopener noreferrer" class="inline-block px-6 py-3 text-base font-semibold rounded-lg bg-white text-brand-600 hover:bg-gray-100">GitHub ↗</a>
      </div>
    </div>
  </section>

  <!-- Main -->
  <main id="main">
    <!-- Introduction -->
    <section id="introduction" class="max-w-5xl mx-auto px-4 py-16">
      <h2 class="text-3xl font-bold text-gray-900 dark:text-gray-100 mb-6 text-center">Introduction</h2>

      <p class="text-gray-700 dark:text-gray-300 leading-relaxed mb-6 text-center max-w-3xl mx-auto">
        The <strong>nano4M</strong> project, developed for the COM-304 course at EPFL, is a minimalist multimodal
        model inspired by 4M. It excels in diverse tasks, supports fine-tuning for specialized applications, and enables flexible any-to-any multimodal generation.
      </p>
      <p class="text-gray-700 dark:text-gray-300 leading-relaxed text-center max-w-3xl mx-auto">
        Our work includes a causal Transformer for text generation, an encoder-only Transformer for image generation,
        and a multimodal encoder-decoder for tasks like in-painting and cross-modal generation.
      </p>
      <p class="text-gray-700 dark:text-gray-300 leading-relaxed text-center max-w-3xl mx-auto">
        For the extensions, our initial goal was to synchronize audio and video generation and implement tokenizers from scratch.
      </p>
      <p class="text-gray-700 dark:text-gray-300 leading-relaxed text-center max-w-3xl mx-auto">
        This page summarises the pipeline and the early—still rough—results.
      </p>
    </section>

    <!-- Capabilities -->
    <section id="capabilities" class="py-16">
      <div class="max-w-7xl mx-auto px-4">
        <h2 class="text-3xl font-bold text-gray-900 dark:text-gray-100 mb-10 text-center">Model Capabilities</h2>
        <div class="grid grid-cols-1 md:grid-cols-3 gap-6">
          <div class="card-hover bg-white dark:bg-gray-900 p-6 rounded-xl shadow-lg">
            <h3 class="text-xl font-semibold text-gray-900 dark:text-gray-100 mb-3">Text Generation</h3>
            <p class="text-gray-700 dark:text-gray-300">A decoder-only Transformer enables autoregressive text generation, trained on a simple dataset to produce coherent sequences.</p>
          </div>
          <div class="card-hover bg-white dark:bg-gray-900 p-6 rounded-xl shadow-lg">
            <h3 class="text-xl font-semibold text-gray-900 dark:text-gray-100 mb-3">Image Generation</h3>
            <p class="text-gray-700 dark:text-gray-300">An encoder-only Transformer supports autoregressive and masked image generation, enabling tasks like inpainting.</p>
          </div>
          <div class="card-hover bg-white dark:bg-gray-900 p-6 rounded-xl shadow-lg">
            <h3 class="text-xl font-semibold text-gray-900 dark:text-gray-100 mb-3">Multimodal Generation</h3>
            <p class="text-gray-700 dark:text-gray-300">An encoder-decoder model enables any-to-any generation across text and images.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Extensions -->
    <section id="extensions" class="bg-gray-50 dark:bg-gray-800 py-16">
      <div class="max-w-6xl mx-auto px-4">
        <h2 class="text-3xl font-bold text-gray-900 dark:text-gray-100 mb-10 text-center">Extensions</h2>
        
        <div class="space-y-16">
          <!-- Dataset Pipeline -->
          <article class="space-y-6">
            <h3 class="text-2xl font-semibold text-gray-900 dark:text-gray-100">Multimodal Dataset & Tokenisers</h3>
            <p class="text-gray-700 dark:text-gray-300 max-w-4xl leading-relaxed">
            <p class="text-gray-700 dark:text-gray-300 max-w-4xl leading-relaxed">
             
            </p>
            
            <figure class="mt-6">
              <img src="pipeline2.jpg" loading="lazy" alt="Dataset & tokenisation diagram" class="rounded-xl shadow-md mx-auto max-w-full" />
              <figcaption class="text-sm text-gray-500 dark:text-gray-400 mt-2 text-center">Comprehensive diagram of our data-collection, preprocessing, and tokenisation pipeline showing the flow from raw multimodal data to discrete token representations.</figcaption>
            </figure>
            

          <!-- Cross-Modal Generation Results -->
        <section id="results" class="py-16"></section>
            <div class="max-w-7xl mx-auto px-4">
              <h2 class="text-3xl font-bold text-gray-900 dark:text-gray-100 mb-10 text-center">Results & Performance Analysis</h2>
        
          <article>
            <h3 class="text-2xl font-semibold text-gray-900 dark:text-gray-100 mb-8">Cross-Modal Generation Capabilities</h3>
            <p class="text-gray-700 dark:text-gray-300 max-w-4xl leading-relaxed mb-8">
            </p>
            
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-12">
              <!-- RGB to Video -->
              <div class="bg-white dark:bg-gray-900 rounded-xl p-8 shadow-lg">
                <h4 class="text-xl font-semibold text-gray-900 dark:text-gray-100 mb-6 text-center">RGB → Video Generation</h4>
                <p class="text-sm text-gray-700 dark:text-gray-300 mb-6 text-center">
                  Temporal interpolation and motion synthesis from static images using learned motion priors.
                </p>
                <div class="flex items-center justify-center gap-8">
                  <div class="text-center">
                    <div class="w-48 h-36 rounded-lg mx-auto mb-3 overflow-hidden bg-gray-200 dark:bg-gray-700 shadow-md">
                      <img src="images/pecheur.png" alt="RGB Input Image" class="w-full h-full object-cover" />
                    </div>
                    <p class="text-base font-medium text-gray-700 dark:text-gray-300">Input Image</p>
                    <p class="text-xs text-gray-500 dark:text-gray-400">Single frame</p>
                  </div>
                  <div class="text-3xl text-gray-400 font-light">→</div>
                  <div class="text-center">
                    <div class="w-48 h-36 rounded-lg mx-auto mb-3 overflow-hidden bg-gray-200 dark:bg-gray-700 shadow-md">
                      <video src="video/pecheur_rec.mp4" class="w-full h-full object-cover" muted loop autoplay></video>
                    </div>
                    <p class="text-base font-medium text-gray-700 dark:text-gray-300">Generated Video</p>
                    <p class="text-xs text-gray-500 dark:text-gray-400">2.5s sequence</p>
                  </div>
                </div>
                <div class="mt-4 text-xs text-gray-600 dark:text-gray-400 bg-gray-50 dark:bg-gray-800 p-3 rounded">
                  <strong>Technical Note:</strong> Video generation uses temporal conditioning with learned motion embeddings, achieving 24 FPS output with smooth transitions.
                </div>
              </div>

              <!-- Video to RGB -->
              <div class="bg-white dark:bg-gray-900 rounded-xl p-8 shadow-lg">
                <h4 class="text-xl font-semibold text-gray-900 dark:text-gray-100 mb-6 text-center">Video → RGB Extraction</h4>
                <p class="text-sm text-gray-700 dark:text-gray-300 mb-6 text-center">
                  Intelligent keyframe selection and temporal averaging for optimal static representation.
                </p>
                <div class="flex items-center justify-center gap-8">
                  <div class="text-center">
                    <div class="w-48 h-36 rounded-lg mx-auto mb-3 overflow-hidden bg-gray-200 dark:bg-gray-700 shadow-md">
                      <video src="video/pecheur.mp4" class="w-full h-full object-cover" muted loop autoplay></video>
                    </div>
                    <p class="text-base font-medium text-gray-700 dark:text-gray-300">Input Video</p>
                    <p class="text-xs text-gray-500 dark:text-gray-400">Multi-frame input</p>
                  </div>
                  <div class="text-3xl text-gray-400 font-light">→</div>
                  <div class="text-center">
                    <div class="w-48 h-36 rounded-lg mx-auto mb-3 overflow-hidden bg-gray-200 dark:bg-gray-700 shadow-md">
                      <img src="images/pecheur_flou.png" alt="Extracted RGB Image" class="w-full h-full object-cover" />
                    </div>
                    <p class="text-base font-medium text-gray-700 dark:text-gray-300">RGB Output</p>
                    <p class="text-xs text-gray-500 dark:text-gray-400">Optimized frame</p>
                  </div>
                </div>
                <div class="mt-4 text-xs text-gray-600 dark:text-gray-400 bg-gray-50 dark:bg-gray-800 p-3 rounded">
                  <strong>Algorithm:</strong> Attention-weighted temporal pooling identifies most representative frames based on content saliency.
                </div>
              </div>

              <!-- RGB to Depth -->
              <div class="bg-white dark:bg-gray-900 rounded-xl p-8 shadow-lg">
                <h4 class="text-xl font-semibold text-gray-900 dark:text-gray-100 mb-6 text-center">RGB → Depth Estimation</h4>
                <p class="text-sm text-gray-700 dark:text-gray-300 mb-6 text-center">
                  Monocular depth estimation using learned geometric priors and spatial attention mechanisms.
                </p>
                <div class="flex items-center justify-center gap-8">
                  <div class="text-center">
                    <div class="w-48 h-36 rounded-lg mx-auto mb-3 overflow-hidden bg-gray-200 dark:bg-gray-700 shadow-md">
                      <img src="images/tambour.jpg" alt="RGB Input Image" class="w-full h-full object-cover" />
                    </div>
                    <p class="text-base font-medium text-gray-700 dark:text-gray-300">RGB Image</p>
                    <p class="text-xs text-gray-500 dark:text-gray-400">Original scene</p>
                  </div>
                  <div class="text-3xl text-gray-400 font-light">→</div>
                  <div class="text-center">
                    <div class="w-48 h-36 rounded-lg mx-auto mb-3 overflow-hidden bg-gray-200 dark:bg-gray-700 shadow-md">
                      <img src="images/tambour_depth.jpg" alt="Generated Depth Map" class="w-full h-full object-cover" />
                    </div>
                    <p class="text-base font-medium text-gray-700 dark:text-gray-300">Depth Map</p>
                    <p class="text-xs text-gray-500 dark:text-gray-400">Estimated depth</p>
                  </div>
                </div>
                <div class="mt-4 text-xs text-gray-600 dark:text-gray-400 bg-gray-50 dark:bg-gray-800 p-3 rounded">
                  <strong>Performance:</strong> Mean relative error: 0.127, achieving competitive results compared to DepthAnything baseline.
                </div>
              </div>

              <!-- Depth to RGB -->
              <div class="bg-white dark:bg-gray-900 rounded-xl p-8 shadow-lg">
                <h4 class="text-xl font-semibold text-gray-900 dark:text-gray-100 mb-6 text-center">Depth → RGB Reconstruction</h4>
                <p class="text-sm text-gray-700 dark:text-gray-300 mb-6 text-center">
                  Novel 3D-aware image synthesis using depth conditioning and learned texture priors.
                </p>
                <div class="flex items-center justify-center gap-8">
                  <div class="text-center">
                    <div class="w-48 h-36 rounded-lg mx-auto mb-3 overflow-hidden bg-gray-200 dark:bg-gray-700 shadow-md">
                      <img src="images/pecheur_depth.png" alt="Depth Map Input" class="w-full h-full object-cover" />
                    </div>
                    <p class="text-base font-medium text-gray-700 dark:text-gray-300">Depth Map</p>
                    <p class="text-xs text-gray-500 dark:text-gray-400">Input geometry</p>
                  </div>
                  <div class="text-3xl text-gray-400 font-light">→</div>
                  <div class="text-center">
                    <div class="w-48 h-36 rounded-lg mx-auto mb-3 overflow-hidden bg-gray-200 dark:bg-gray-700 shadow-md">
                      <img src="images/recon_pecheur.png" alt="Generated RGB Image" class="w-full h-full object-cover" />
                    </div>
                    <p class="text-base font-medium text-gray-700 dark:text-gray-300">Reconstructed RGB</p>
                    <p class="text-xs text-gray-500 dark:text-gray-400">Synthesized image</p>
                  </div>
                </div>
                <div class="mt-4 text-xs text-gray-600 dark:text-gray-400 bg-gray-50 dark:bg-gray-800 p-3 rounded">
                  <strong>Innovation:</strong> First end-to-end depth-to-RGB generation in nano4M framework, using geometric awareness for realistic texturing.
                </div>
              </div>
            </div>
          </article>
        </div>
      </div>
    </section>


          <!-- Limitations & Challenges -->
          <div class="bg-white dark:bg-gray-900 rounded-xl p-8 shadow-lg">
            <h3 class="text-2xl font-semibold text-gray-900 dark:text-gray-100 mb-6">Limitations & Challenges</h3>
            <div class="space-y-6">
              <div>
                <h4 class="text-lg font-semibold text-gray-900 dark:text-gray-100 mb-3">Current Limitations</h4>
                <ul class="space-y-2 text-gray-700 dark:text-gray-300">
                  <li class="flex items-start">
                    <span class="text-red-500 mr-2">•</span>
                    <span><strong>Dataset Heterogeneity:</strong> VGGSound's diverse content makes training challenging, leading to mode collapse in some generation tasks.</span>
                  </li>
                  <li class="flex items-start">
                    <span class="text-red-500 mr-2">•</span>
                    <span><strong>Limited Resolution:</strong> Current implementation restricted to 256×256 images due to computational constraints.</span>
                  </li>
                  <li class="flex items-start">
                    <span class="text-red-500 mr-2">•</span>
                    <span><strong>Temporal Consistency:</strong> Video generation occasionally exhibits temporal artifacts and inconsistent object motion.</span>
                  </li>
                  <li class="flex items-start">
                    <span class="text-red-500 mr-2">•</span>
                    <span><strong>Audio Quality:</strong> Compression introduces perceptible artifacts in high-frequency audio content.</span>
                  </li>
                </ul>
              </div>
              
              <div>
                <h4 class="text-lg font-semibold text-gray-900 dark:text-gray-100 mb-3">Technical Challenges Encountered</h4>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                  <div class="p-4 bg-gray-50 dark:bg-gray-800 rounded">
                    <h5 class="font-medium text-gray-900 dark:text-gray-100 mb-2">Training Stability</h5>
                    <p class="text-sm text-gray-700 dark:text-gray-300">Multimodal training exhibited significant variance in loss convergence, requiring careful learning rate scheduling and gradient clipping strategies.</p>
                  </div>
                  <div class="p-4 bg-gray-50 dark:bg-gray-800 rounded">
                    <h5 class="font-medium text-gray-900 dark:text-gray-100 mb-2">Memory Constraints</h5>
                    <p class="text-sm text-gray-700 dark:text-gray-300">Processing multimodal sequences required extensive optimization to fit within available GPU memory, limiting batch sizes and model capacity.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- W&B Dashboard -->
    <section class="bg-gray-50 dark:bg-gray-800 py-16">
      <div class="max-w-7xl mx-auto px-4">
        <h3 class="text-2xl font-semibold text-gray-900 dark:text-gray-100 mb-6 text-center">Interactive Training Dashboard</h3>
        <p class="text-center text-gray-700 dark:text-gray-300 mb-8 max-w-3xl mx-auto">
         Training Dashboards
        </p>
        <iframe src="https://api.wandb.ai/links/SAGA_COM-304/p5nttx6y" title="WandB Project Dashboard" style="border:none;width:100%;height:640px" loading="lazy" allowfullscreen></iframe>
        <figcaption class="text-sm text-gray-500 dark:text-gray-400 mt-4 text-center">Interactive dashboard: hover over data points, zoom into specific training phases, or hide/show individual metrics to analyze training progression in detail.</figcaption>
      </div>
    </section>

    <!-- Conclusion -->
    <section id="conclusion" class="max-w-5xl mx-auto px-4 py-16">
      <h2 class="text-3xl font-bold text-gray-900 dark:text-gray-100 mb-6 text-center">Conclusion & Future Improvements</h2>
      
      <div class="space-y-8">
        <div class="text-center">
          <p class="text-gray-700 dark:text-gray-300 leading-relaxed max-w-4xl mx-auto mb-6">
            The nano4M now adds audio and video to the any - to - any 4M model. Results for now aren't as good as wanted but we propose different approches in order to generate better ones.
          </p>
          <p class="text-gray-700 dark:text-gray-300 leading-relaxed max-w-4xl mx-auto">
           
          </p>
        </div>
        
        <div class="bg-white dark:bg-gray-900 rounded-xl p-8 shadow-lg">
          <h3 class="text-xl font-semibold text-gray-900 dark:text-gray-100 mb-6 text-center">Proposed Improvement Pathways</h3>
          
          <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
            <div>
              <h4 class="text-lg font-semibold text-gray-900 dark:text-gray-100 mb-4 flex items-center">
                <span class="w-2 h-2 bg-blue-500 rounded-full mr-3"></span>
                Short-term Improvements
              </h4>
              <ul class="space-y-3 text-gray-700 dark:text-gray-300">
                <li class="flex items-start">
                  <span class="text-blue-500 mr-2 mt-1">▸</span>
                  <div>
                    <strong>Dataset Curation:</strong> Implement more selective filtering and create domain-specific subsets to reduce training complexity and improve convergence stability.
                  </div>
                </li>
                <li class="flex items-start">
                  <span class="text-blue-500 mr-2 mt-1">▸</span>
                  <div>
                    <strong>Resolution Scaling:</strong> Implement progressive resolution training to achieve higher quality outputs while maintaining training efficiency.
                  </div>
                </li>
                <li class="flex items-start">
                  <span class="text-blue-500 mr-2 mt-1">▸</span>
                  <div>
                    <strong>Tokenizer Optimization:</strong> Fine-tune compression ratios and develop modality-specific quantization schemes for better reconstruction quality.
                  </div>
                </li>
              </ul>
            </div>
            
            <div>
              <h4 class="text-lg font-semibold text-gray-900 dark:text-gray-100 mb-4 flex items-center">
                <span class="w-2 h-2 bg-green-500 rounded-full mr-3"></span>
                Long-term Research Directions
              </h4>
              <ul class="space-y-3 text-gray-700 dark:text-gray-300">
                <li class="flex items-start">
                  <span class="text-green-500 mr-2 mt-1">▸</span>
                  <div>
                    <strong>Architecture Innovation:</strong> Explore attention mechanisms specifically designed for multimodal processing and cross-modal alignment.
                  </div>
                </li>
                <li class="flex items-start">
                  <span class="text-green-500 mr-2 mt-1">▸</span>
                  <div>
                    <strong>Unified Training:</strong> Develop end-to-end training strategies that jointly optimize all modalities from the beginning of training.
                  </div>
                </li>
                <li class="flex items-start">
                  <span class="text-green-500 mr-2 mt-1">▸</span>
                  <div>
                    <strong>Real-time Applications:</strong> Optimize inference pipeline for interactive applications and real-time multimodal generation.
                  </div>
                </li>
              </ul>
            </div>
          </div>
          
          <div class="mt-8 p-6 bg-gradient-to-r from-brand-50 to-blue-50 dark:from-brand-900/20 dark:to-blue-900/20 rounded-lg">
            <h4 class="text-lg font-semibold text-gray-900 dark:text-gray-100 mb-3">Recommended Next Steps</h4>
            <p class="text-gray-700 dark:text-gray-300 leading-relaxed">
              The most promising direction for immediate improvement would be to experiment with smaller, more curated datasets that exhibit greater coherence and consistency. This approach would allow for better model understanding and more stable training dynamics, potentially unlocking superior performance even with our current architectural constraints. Additionally, implementing hierarchical tokenization strategies could significantly improve both training efficiency and output quality across all modalities.
            </p>
          </div>
        </div>
      </div>
    </section>

    <!-- References -->
    <section id="references" class="bg-gray-50 dark:bg-gray-800 py-16">
      <div class="max-w-5xl mx-auto px-4">
        <h2 class="text-3xl font-bold text-gray-900 dark:text-gray-100 mb-8 text-center">References</h2>
        <div class="bg-white dark:bg-gray-900 rounded-xl p-8 shadow-lg">
          <ol class="space-y-4 text-gray-700 dark:text-gray-300 text-sm">
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[1]</span>
              <span>Bachmann, R., Mizrahi, O., Atanov, A., and Zamir, A. <em>4M-21: An any-to-any model for tens of tasks and modalities</em>. In Advances in Neural Information Processing Systems (NeurIPS), 2024.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[2]</span>
              <span>Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M. H., Murphy, K., Freeman, W. T., Rubinstein, M., Li, Y., and Krishnan, D. <em>Muse: Text-to-image generation via masked generative transformers</em>. arXiv preprint arXiv:2301.00704, 2023.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[3]</span>
              <span>Karpathy, A. <em>nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs</em>. GitHub repository. <a href="https://github.com/karpathy/nanoGPT" class="text-brand-600 hover:text-brand-800 underline">https://github.com/karpathy/nanoGPT</a>, 2023.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[4]</span>
              <span>Défossez, A., Caucheteux, C., Rapin, J., Adi, Y., and Synnaeve, G. <em>Moshi: a speech-text foundation model for real-time dialogue</em>. arXiv preprint arXiv:2410.00037, October 2024.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[5]</span>
              <span>NVIDIA AI, Liang, W., Yang, Z., and team. <em>Cosmos World Foundation Model Platform for Physical AI</em>. arXiv preprint arXiv:2501.03575, January 2025.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[6]</span>
              <span>Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., and Zhao, H. <em>Depth Anything V2</em>. GitHub repository. <a href="https://github.com/DepthAnything/Depth-Anything-V2" class="text-brand-600 hover:text-brand-800 underline">https://github.com/DepthAnything/Depth-Anything-V2</a>, 2024.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[7]</span>
              <span>Chen, H., Xie, W., Vedaldi, A., and Zisserman, A. <em>VGGSound: A Large-scale Audio-Visual Dataset</em>. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 721–725, 2020. arXiv:2004.14368.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[8]</span>
              <span>Ji, S., Jiang, Z., Wang, W., Chen, Y., Fang, C., Zhou, J., and Xu, Y. <em>WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling</em>. arXiv preprint arXiv:2408.16532, February 2025.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[9]</span>
              <span>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. <em>Attention is all you need</em>. In Advances in Neural Information Processing Systems (NIPS), pp. 5998–6008, 2017.</span>
            </li>
            <li class="flex">
              <span class="text-gray-500 dark:text-gray-400 mr-4 mt-0.5">[10]</span>
              <span>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. <em>Language models are unsupervised multitask learners</em>. OpenAI Technical Report, 2019.</span>
            </li>
          </ol>
        </div>
      </div>
    </section>
  </main>

  <!-- Footer -->
  <footer class="bg-brand-600 text-white py-8 text-center">
    <div class="max-w-4xl mx-auto">
      <p class="text-sm mb-2">
        Developed by <span class="font-semibold">SAGA</span> | COM-304 Spring 2025
      </p>
      <p class="text-xs text-brand-100">
        <a href="https://github.com/SAGA-COM-304/SAGA_COM-304" target="_blank" rel="noopener noreferrer" class="underline hover:text-white">GitHub Repository ↗</a> | 
        <a href="https://www.epfl.ch" target="_blank" rel="noopener noreferrer" class="underline hover:text-white">EPFL</a> | 
        École Polytechnique Fédérale de Lausanne
      </p>
    </div>
  </footer>

  <!-- Tiny JS (dark-mode + mobile nav) -->
  <script>
    const darkToggle=document.getElementById('dark-toggle'),sun=document.getElementById('icon-sun'),moon=document.getElementById('icon-moon'),
          prefersDark=window.matchMedia('(prefers-color-scheme: dark)').matches,
          saved=localStorage.getItem('theme'),isDark=saved?saved==='dark':prefersDark;
    document.documentElement.classList.toggle('dark',isDark);moon.classList.toggle('hidden',!isDark);sun.classList.toggle('hidden',isDark);
    darkToggle.addEventListener('click',()=>{const d=document.documentElement.classList.toggle('dark');
      localStorage.setItem('theme',d?'dark':'light');moon.classList.toggle('hidden',!d);sun.classList.toggle('hidden',d);});
    const menu=document.getElementById('menu-button'),mobile=document.getElementById('mobile-nav');
    menu.addEventListener('click',()=>{const open=mobile.style.maxHeight&&mobile.style.maxHeight!=='0px';
      mobile.style.maxHeight=open?'0':mobile.scrollHeight+'px';});
  </script>
</body>
</html>